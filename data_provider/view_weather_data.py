import os
import warnings
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
warnings.filterwarnings('ignore')

# -------------------------- æ ¸å¿ƒé…ç½®ï¼ˆç›´æ¥ä¿®æ”¹è¿™é‡Œï¼Œå’Œé¡¹ç›®configå¯¹é½ï¼‰--------------------------
# æ•°æ®é…ç½®
DATA_PATH = "./datasets/weather.csv"  # æ•°æ®é›†è·¯å¾„ï¼ˆå’Œé¡¹ç›®ä¸€è‡´ï¼‰
DATASET = "weather"                  # æ•°æ®é›†åç§°
# æ—¶åºå‚æ•°
SEQ_LEN = 96                         # è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå†å²æ—¶é—´æ­¥ï¼‰
PRED_LEN = 24                        # é¢„æµ‹åºåˆ—é•¿åº¦ï¼ˆæœªæ¥æ—¶é—´æ­¥ï¼‰
# æ•°æ®åˆ’åˆ†
SPLITER_RATIO = "7:2:1"              # è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ¯”ä¾‹
BATCH_SIZE = 32                      # DataLoaderæ‰¹æ¬¡å¤§å°
# å…¶ä»–é…ç½®
TS_VAR = 1                           # å‚è€ƒé¡¹ç›®get_tsï¼š1=é¢„æµ‹æ‰€æœ‰ç‰¹å¾ï¼Œ0=é¢„æµ‹æœ€åä¸€åˆ—
SHUFFLE_TRAIN = False                # è®­ç»ƒé›†æ˜¯å¦æ‰“ä¹±ï¼ˆæŸ¥çœ‹æ•°æ®æ—¶è®¾ä¸ºFalseï¼Œä¿æŒæ—¶åºï¼‰
PRINT_ROWS = 3                       # æ¯ä¸ªé˜¶æ®µæ‰“å°å‰3è¡Œ
# ----------------------------------------------------------------------------------

class MockConfig:
    """æ¨¡æ‹Ÿé¡¹ç›®çš„Configå¯¹è±¡ï¼ˆæ— éœ€å¯¼å…¥get_configï¼Œç›´æ¥åœ¨è¿™å®šä¹‰å‚æ•°ï¼‰"""
    def __init__(self):
        self.dataset = DATASET
        self.seq_len = SEQ_LEN
        self.pred_len = PRED_LEN
        self.bs = BATCH_SIZE
        self.spliter_ratio = SPLITER_RATIO
        self.ts_var = TS_VAR
        self.shuffle = SHUFFLE_TRAIN
        self.use_train_size = False
        self.train_size = 0
        self.eval_set = True
        self.classification = False
        self.debug = False
        
        # æ¨¡æ‹Ÿæ—¥å¿—ï¼ˆä»…æ‰“å°å…³é”®ä¿¡æ¯ï¼‰
        class SimpleLog:
            def only_print(self, msg):
                print(f"ğŸ“Œ {msg}")
        self.log = SimpleLog()

def get_ts(data_path, config):
    """å¤ç°é¡¹ç›® get_ts é€»è¾‘ï¼šè¯»å–æ•°æ®+æå–æ—¶é—´ç‰¹å¾+å½’ä¸€åŒ–"""
    # è¯»å–CSVæ•°æ®
    df = pd.read_csv(data_path).to_numpy()
    if config.ts_var == 1:
        x, y = df[:, 1:], df[:, 1:]  # æ‰€æœ‰ç‰¹å¾ä½œä¸ºè¾“å…¥å’Œç›®æ ‡
    else:
        x, y = df[:, -1:], df[:, -1:]  # ä»…æœ€åä¸€åˆ—ä½œä¸ºè¾“å…¥å’Œç›®æ ‡
    
    # æå–æ—¶é—´ç‰¹å¾ï¼ˆæœˆ/æ—¥/å‘¨/æ—¶ï¼‰
    timestamps = pd.to_datetime(df[:, 0])
    time_features = np.array([[ts.month, ts.day, ts.weekday(), ts.hour] for ts in timestamps])
    
    # å½’ä¸€åŒ–ï¼ˆå¤ç°é¡¹ç›® get_scaler é€»è¾‘ï¼Œç”¨MinMaxScaler 0-1ç¼©æ”¾ï¼‰
    def minmax_scaler(data):
        scaler = MinMaxScaler(feature_range=(0, 1))
        return scaler.fit_transform(data), scaler
    
    x_scaled, x_scaler = minmax_scaler(x)
    y_scaled, y_scaler = minmax_scaler(y)
    
    # æ‹¼æ¥æ—¶é—´ç‰¹å¾å’Œæ•°å€¼ç‰¹å¾ï¼ˆxåŒ…å«ï¼š4ä¸ªæ—¶é—´ç‰¹å¾ + Nä¸ªå¤©æ°”ç‰¹å¾ï¼‰
    x_combined = np.concatenate((time_features, x_scaled), axis=1).astype(np.float32)
    y_scaled = y_scaled.astype(np.float32)
    
    return x_combined, y_scaled, x_scaler, y_scaler

def parse_split_ratio(ratio_str):
    """å¤ç°é¡¹ç›® parse_split_ratio é€»è¾‘ï¼šè§£æ7:2:1ä¸ºæ¯”ä¾‹"""
    parts = list(map(int, ratio_str.strip().split(':')))
    total = sum(parts)
    return [p / total for p in parts]

def get_train_valid_test_dataset(x, y, config):
    """å¤ç°é¡¹ç›®æ•°æ®åˆ’åˆ†é€»è¾‘ï¼šæŒ‰æ¯”ä¾‹åˆ’åˆ†ï¼Œæ—¶åºä¸æ‰“ä¹±"""
    train_ratio, valid_ratio, _ = parse_split_ratio(config.spliter_ratio)
    
    # è®¡ç®—åˆ’åˆ†å¤§å°
    if config.use_train_size:
        train_size = int(config.train_size)
    else:
        train_size = int(len(x) * train_ratio)
    
    valid_size = int(len(x) * valid_ratio) if config.eval_set else 0
    
    # æ—¶åºæ•°æ®ä¸æ‰“ä¹±ï¼ˆä¿æŒé¡ºåºï¼‰
    train_x = x[:train_size]
    train_y = y[:train_size]
    valid_x = x[train_size:train_size + valid_size]
    valid_y = y[train_size:train_size + valid_size]
    test_x = x[train_size + valid_size:]
    test_y = y[train_size + valid_size:]
    
    return train_x, train_y, valid_x, valid_y, test_x, test_y

class TimeSeriesDataset(Dataset):
    """å¤ç°é¡¹ç›® TimeSeriesDataset é€»è¾‘ï¼šæ„å»ºæ—¶åºè¾“å…¥è¾“å‡ºå¯¹"""
    def __init__(self, x, y, mode, config):
        self.x = x  # å½¢çŠ¶ï¼š(æ—¶é—´æ­¥, 4+Nç‰¹å¾)
        self.y = y  # å½¢çŠ¶ï¼š(æ—¶é—´æ­¥, Nç‰¹å¾)
        self.config = config
        self.mode = mode
    
    def __len__(self):
        """æ ·æœ¬æ•° = æ€»æ—¶é—´æ­¥ - è¾“å…¥é•¿åº¦ - é¢„æµ‹é•¿åº¦ + 1"""
        return len(self.x) - self.config.seq_len - self.config.pred_len + 1
    
    def __getitem__(self, idx):
        """æ»‘åŠ¨çª—å£å–æ•°æ®ï¼šxå–å¤©æ°”ç‰¹å¾ï¼Œx_markå–æ—¶é—´ç‰¹å¾"""
        s_begin = idx
        s_end = s_begin + self.config.seq_len
        r_begin = s_end
        r_end = r_begin + self.config.pred_len
        
        # xï¼šå¤©æ°”ç‰¹å¾ï¼ˆå»æ‰å‰4ä¸ªæ—¶é—´ç‰¹å¾ï¼‰â†’ (seq_len, Nç‰¹å¾)
        x = self.x[s_begin:s_end][:, 4:]
        # x_markï¼šæ—¶é—´ç‰¹å¾ï¼ˆå‰4åˆ—ï¼šæœˆ/æ—¥/å‘¨/æ—¶ï¼‰â†’ (seq_len, 4)
        x_mark = self.x[s_begin:s_end][:, :4]
        # yï¼šç›®æ ‡å€¼ â†’ (pred_len, Nç‰¹å¾)
        y = self.y[r_begin:r_end]
        
        return torch.tensor(x), torch.tensor(x_mark), torch.tensor(y)
    
    @staticmethod
    def custom_collate_fn(batch):
        """å¤ç°é¡¹ç›® collate_fn é€»è¾‘ï¼šæ‰¹é‡å¤„ç†æ•°æ®"""
        x, x_mark, y = zip(*batch)
        x = torch.stack(x)
        x_mark = torch.stack(x_mark)
        y = torch.stack(y)
        return x, x_mark, y

def get_dataloaders(train_set, valid_set, test_set, config):
    """å¤ç°é¡¹ç›® DataLoader åˆ›å»ºé€»è¾‘ï¼šé€‚é…ç³»ç»Ÿè®¾ç½®å¤šçº¿ç¨‹"""
    import platform
    import multiprocessing
    
    # æ ¹æ®ç³»ç»Ÿè®¾ç½®workeræ•°ï¼ˆé¿å…WindowsæŠ¥é”™ï¼‰
    if platform.system() == 'Linux' and 'ubuntu' in platform.version().lower():
        max_workers = multiprocessing.cpu_count() // 5
        prefetch_factor = 2
    else:
        max_workers = 0
        prefetch_factor = None
    
    # è®­ç»ƒé›†DataLoaderï¼ˆå¯é€‰æ‰“ä¹±ï¼‰
    train_loader = DataLoader(
        train_set,
        batch_size=config.bs,
        shuffle=config.shuffle,
        drop_last=False,
        pin_memory=True,
        collate_fn=TimeSeriesDataset.custom_collate_fn,
        num_workers=max_workers,
        prefetch_factor=prefetch_factor
    )
    
    # éªŒè¯é›†/æµ‹è¯•é›†ä¸æ‰“ä¹±
    valid_loader = DataLoader(
        valid_set,
        batch_size=config.bs,
        shuffle=False,
        drop_last=False,
        pin_memory=True,
        collate_fn=TimeSeriesDataset.custom_collate_fn,
        num_workers=max_workers,
        prefetch_factor=prefetch_factor
    )
    
    test_loader = DataLoader(
        test_set,
        batch_size=config.bs,
        shuffle=False,
        drop_last=False,
        pin_memory=True,
        collate_fn=TimeSeriesDataset.custom_collate_fn,
        num_workers=max_workers,
        prefetch_factor=prefetch_factor
    )
    
    return train_loader, valid_loader, test_loader

def print_raw_data_info(x, y):
    """æ‰“å°åŸå§‹æ•°æ®ï¼ˆget_tså¤„ç†åï¼‰å…³é”®ä¿¡æ¯"""
    print("="*60)
    print("ã€1. åŸå§‹æ•°æ®ï¼ˆé¢„å¤„ç†åï¼‰ã€‘")
    print("="*60)
    print(f"æ•°æ®å½¢çŠ¶ï¼šx={x.shape} (æ—¶é—´æ­¥ Ã— ç‰¹å¾æ•°)ï¼Œy={y.shape} (æ—¶é—´æ­¥ Ã— ç›®æ ‡ç‰¹å¾æ•°)")
    print(f"ç‰¹å¾æ„æˆï¼š4ä¸ªæ—¶é—´ç‰¹å¾ï¼ˆæœˆ/æ—¥/å‘¨/æ—¶ï¼‰ + {x.shape[1]-4}ä¸ªå¤©æ°”ç‰¹å¾")
    print(f"æ€»æ—¶é—´æ­¥ï¼š{x.shape[0]}")
    
    # æ‰“å°å‰3è¡Œï¼ˆæ—¶é—´ç‰¹å¾+å¤©æ°”ç‰¹å¾å‰5åˆ—ï¼‰
    time_cols = ["month", "day", "weekday", "hour"]
    weather_cols = [f"weather_feat_{i}" for i in range(5)]  # åªæ˜¾ç¤ºå‰5ä¸ªå¤©æ°”ç‰¹å¾
    x_display = pd.DataFrame(x[:PRINT_ROWS, :4+5], columns=time_cols + weather_cols)
    print(f"\nå‰{PRINT_ROWS}è¡Œæ•°æ®ï¼ˆæ—¶é—´ç‰¹å¾+å‰5ä¸ªå¤©æ°”ç‰¹å¾ï¼‰ï¼š")
    print(x_display.round(4))
    print()

def print_split_info(train_x, train_y, valid_x, valid_y, test_x, test_y):
    """æ‰“å°åˆ’åˆ†åæ•°æ®é›†ä¿¡æ¯"""
    print("="*60)
    print("ã€2. æ•°æ®é›†åˆ’åˆ†ç»“æœã€‘")
    print("="*60)
    print(f"è®­ç»ƒé›†ï¼š{train_x.shape[0]}ä¸ªæ—¶é—´æ­¥ â†’ æ ·æœ¬æ•°ï¼š{len(TimeSeriesDataset(train_x, train_y, 'train', config))}")
    print(f"éªŒè¯é›†ï¼š{valid_x.shape[0]}ä¸ªæ—¶é—´æ­¥ â†’ æ ·æœ¬æ•°ï¼š{len(TimeSeriesDataset(valid_x, valid_y, 'valid', config))}")
    print(f"æµ‹è¯•é›†ï¼š{test_x.shape[0]}ä¸ªæ—¶é—´æ­¥ â†’ æ ·æœ¬æ•°ï¼š{len(TimeSeriesDataset(test_x, test_y, 'test', config))}")
    print()

def print_dataloader_batch(loader, mode="è®­ç»ƒé›†"):
    """æ‰“å°DataLoaderæ‰¹æ¬¡ï¼ˆæ ¸å¿ƒï¼šæ¨¡å‹çœŸå®è¾“å…¥æ ¼å¼ï¼‰"""
    print("="*60)
    print(f"ã€3. {mode} DataLoader æ‰¹æ¬¡è¯¦æƒ…ã€‘")
    print("="*60)
    
    # å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
    batch = next(iter(loader))
    x, x_mark, y = batch
    
    print(f"æ‰¹æ¬¡å½¢çŠ¶ï¼ˆbatch_size={config.bs}ï¼‰ï¼š")
    print(f"  - å¤©æ°”ç‰¹å¾ xï¼š{x.shape} (æ‰¹æ¬¡å¤§å° Ã— è¾“å…¥æ—¶é—´æ­¥ Ã— å¤©æ°”ç‰¹å¾æ•°)")
    print(f"  - æ—¶é—´æ ‡è®° x_markï¼š{x_mark.shape} (æ‰¹æ¬¡å¤§å° Ã— è¾“å…¥æ—¶é—´æ­¥ Ã— æ—¶é—´ç‰¹å¾æ•°)")
    print(f"  - é¢„æµ‹ç›®æ ‡ yï¼š{y.shape} (æ‰¹æ¬¡å¤§å° Ã— é¢„æµ‹æ—¶é—´æ­¥ Ã— å¤©æ°”ç‰¹å¾æ•°)")
    
    # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å‰3ä¸ªæ—¶é—´æ­¥
    print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬ - æ—¶é—´æ ‡è®° x_markï¼ˆå‰{PRINT_ROWS}æ­¥ï¼‰ï¼š")
    x_mark_sample = x_mark[0][:PRINT_ROWS].numpy()
    print(pd.DataFrame(x_mark_sample, columns=["month", "day", "weekday", "hour"]).round(0))
    
    print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬ - å¤©æ°”ç‰¹å¾ xï¼ˆå‰{PRINT_ROWS}æ­¥ï¼Œå‰5ä¸ªç‰¹å¾ï¼‰ï¼š")
    x_sample = x[0][:PRINT_ROWS, :5].numpy()
    print(pd.DataFrame(x_sample, columns=[f"feat_{i}" for i in range(5)]).round(4))
    
    print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬ - é¢„æµ‹ç›®æ ‡ yï¼ˆå‰{PRINT_ROWS}æ­¥ï¼Œå‰5ä¸ªç‰¹å¾ï¼‰ï¼š")
    y_sample = y[0][:PRINT_ROWS, :5].numpy()
    print(pd.DataFrame(y_sample, columns=[f"feat_{i}" for i in range(5)]).round(4))
    print()

def main():
    global config
    try:
        # 1. åˆå§‹åŒ–é…ç½®
        config = MockConfig()
        print(f"âœ… é…ç½®åˆå§‹åŒ–å®Œæˆï¼šdataset={config.dataset}ï¼Œseq_len={config.seq_len}ï¼Œpred_len={config.pred_len}")
        print()
        
        # 2. è¯»å–å¹¶é¢„å¤„ç†æ•°æ®ï¼ˆå¤ç°get_tsé€»è¾‘ï¼‰
        if not os.path.exists(DATA_PATH):
            raise FileNotFoundError(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨ï¼š{DATA_PATH}ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
        x, y, x_scaler, y_scaler = get_ts(DATA_PATH, config)
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼ˆè¯»å–+æ—¶é—´ç‰¹å¾æå–+å½’ä¸€åŒ–ï¼‰")
        
        # 3. åˆ’åˆ†æ•°æ®é›†ï¼ˆå¤ç°é¡¹ç›®åˆ’åˆ†é€»è¾‘ï¼‰
        train_x, train_y, valid_x, valid_y, test_x, test_y = get_train_valid_test_dataset(x, y, config)
        
        # 4. åˆ›å»ºDatasetï¼ˆå¤ç°TimeSeriesDatasetï¼‰
        train_set = TimeSeriesDataset(train_x, train_y, 'train', config)
        valid_set = TimeSeriesDataset(valid_x, valid_y, 'valid', config)
        test_set = TimeSeriesDataset(test_x, test_y, 'test', config)
        
        # 5. åˆ›å»ºDataLoaderï¼ˆå¤ç°é¡¹ç›®DataLoaderé€»è¾‘ï¼‰
        train_loader, valid_loader, test_loader = get_dataloaders(train_set, valid_set, test_set, config)
        config.log.only_print(f"DataLoaderåˆ›å»ºå®Œæˆï¼šè®­ç»ƒé›†{len(train_loader)}æ‰¹æ¬¡ï¼ŒéªŒè¯é›†{len(valid_loader)}æ‰¹æ¬¡ï¼Œæµ‹è¯•é›†{len(test_loader)}æ‰¹æ¬¡")
        print()
        
        # 6. æ‰“å°å…³é”®ä¿¡æ¯ï¼ˆç²¾ç®€ç‰ˆï¼‰
        print_raw_data_info(x, y)
        print_split_info(train_x, train_y, valid_x, valid_y, test_x, test_y)
        print_dataloader_batch(train_loader, mode="è®­ç»ƒé›†")
        
        # 7. æ ¸å¿ƒæ€»ç»“
        print("="*60)
        print("ã€æ•°æ®æŸ¥çœ‹æ ¸å¿ƒæ€»ç»“ã€‘")
        print("="*60)
        weather_feat_num = x.shape[1] - 4  # å¤©æ°”ç‰¹å¾æ•°ï¼ˆæ€»ç‰¹å¾æ•°-4ä¸ªæ—¶é—´ç‰¹å¾ï¼‰
        print(f"1. æ¨¡å‹è¾“å…¥ï¼šå¤©æ°”ç‰¹å¾({weather_feat_num}ç»´) + æ—¶é—´ç‰¹å¾(4ç»´)")
        print(f"2. è¾“å…¥æ ¼å¼ï¼š(batch_size, {config.seq_len}, {weather_feat_num})")
        print(f"3. è¾“å‡ºæ ¼å¼ï¼š(batch_size, {config.pred_len}, {weather_feat_num})")
        print(f"4. æ€»æ ·æœ¬æ•°ï¼šè®­ç»ƒé›†{len(train_set)} + éªŒè¯é›†{len(valid_set)} + æµ‹è¯•é›†{len(test_set)} = {len(train_set)+len(valid_set)+len(test_set)}")
        print("âœ… ç‹¬ç«‹è„šæœ¬è¿è¡Œå®Œæˆï¼ˆæ— ä»»ä½•é¡¹ç›®ç»„ä»¶ä¾èµ–ï¼‰")
    
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{str(e)}")

if __name__ == "__main__":
    main()
